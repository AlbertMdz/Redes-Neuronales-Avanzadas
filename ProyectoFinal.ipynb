{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProyectoFinal.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertMdz/Redes-Neuronales-Avanzadas/blob/master/ProyectoFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YrokacAE1-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv0rEZTvWY4Z",
        "colab_type": "code",
        "outputId": "95f7bf08-6349-4fcd-88c6-4b44796aab65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')                             "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtP-9_Q0Y9S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Colab Notebooks/ProyectoFinal/Personality_test.pickle','rb') as f:\n",
        "    train2=pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2Jlqd7IZgLm",
        "colab_type": "code",
        "outputId": "46274d27-f86a-4880-b0eb-5bdafb30768a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "train2[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[132, 110, 101, ...,  88,  76,  73],\n",
              "       [130, 109, 102, ...,  97,  83,  77],\n",
              "       [132, 111, 104, ...,  97,  85,  79],\n",
              "       ...,\n",
              "       [ 62,  63,  67, ..., 237, 238, 244],\n",
              "       [ 65,  68,  72, ..., 241, 242, 242],\n",
              "       [ 67,  70,  73, ..., 242, 242, 242]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuEihsYRrDoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fourpersonalities(personalidad):\n",
        "  traductor= {'Analistas': [1,0,0,0], 'Diplomaticos': [0,1,0,0], 'Centinelas': [0,0,1,0], 'Exploradores': [0,0,0,1]}\n",
        "  try:\n",
        "    y=traductor[personalidad]\n",
        "  except:\n",
        "    y=[0,0,0,0]\n",
        "  return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdlLWgBzdJfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sixteenpersonalities(personalidad):\n",
        "  traductor= {'ESTJ_Ejecutivo': 1, 'INFP-Mediador': 2, 'ENFJ-Protagonista': 3, 'ISTJ-Logista': 4, 'ENFP-Activista': 5, 'ENTJ-Comandante': 6, 'ISFP-Aventurero': 7, 'ESTP-Emprendedor': 8, 'ENTP-Innovador': 9, 'ESFJ-Consul': 10, '': 11}\n",
        "  try:\n",
        "    y=[0]*16\n",
        "    y[traductor[personalidad]]=1\n",
        "  except:\n",
        "    y=[0]*16\n",
        "  return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR1adnO4-p4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reset_graph():\n",
        "    if 'sess' in globals() and sess:\n",
        "        sess.close()\n",
        "    tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdgCfTFMW4mH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Neural_network_model(\n",
        "    keep_rate = 0.8,\n",
        "    n_classes=10,\n",
        "    batch_size=100\n",
        "    ):\n",
        "    reset_graph()    \n",
        "    x=tf.placeholder('float',[None,56,56])\n",
        "    y=tf.placeholder('float')    \n",
        "    img = tf.reshape(x, shape=[-1, 56, 56, 1])\n",
        "    tf.summary.image(\"imagen\",img,max_outputs=30)    \n",
        "    weights = {'W_conv1':tf.Variable(tf.random_normal([5,5,1,16])),\n",
        "               'W_conv2':tf.Variable(tf.random_normal([5,5,16,32])),\n",
        "               'W_conv3':tf.Variable(tf.random_normal([5,5,32,64])),\n",
        "               'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
        "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
        "\n",
        "    biases = {'b_conv1':tf.Variable(tf.random_normal([16])),\n",
        "               'b_conv2':tf.Variable(tf.random_normal([32])),\n",
        "              'b_conv3':tf.Variable(tf.random_normal([64])),\n",
        "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
        "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
        "          \n",
        "    l1 = tf.nn.conv2d(img, weights['W_conv1'], strides=[1,1,1,1], padding='SAME')\n",
        "    l1 = tf.add(l1, biases['b_conv1'])\n",
        "    l1 = tf.nn.relu(l1)\n",
        "    l1 = tf.nn.max_pool(l1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "        \n",
        "    l2 = tf.nn.conv2d(l1, weights['W_conv2'], strides=[1,1,1,1], padding='SAME')\n",
        "    l2 = tf.add(l2, biases['b_conv2'])\n",
        "    l2 = tf.nn.relu(l2)\n",
        "    l2 = tf.nn.max_pool(l2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "    \n",
        "    l3 = tf.nn.conv2d(l2, weights['W_conv3'], strides=[1,1,1,1], padding='SAME')\n",
        "    l3 = tf.add(l3, biases['b_conv3'])\n",
        "    l3 = tf.nn.relu(l3)\n",
        "    l3 = tf.nn.max_pool(l3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "    \n",
        "    fc = tf.reshape(l3,[-1, 7*7*64])\n",
        "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
        "    \n",
        "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
        "\n",
        "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output\n",
        "                                                                   , labels=y) )\n",
        "    tf.summary.scalar(\"cost\",cost)\n",
        "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "    \n",
        "    correct = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "    tf.summary.scalar(\"accuracy\",accuracy)\n",
        "    summaryppal=tf.summary.merge_all()\n",
        "    return dict(\n",
        "              x=x,\n",
        "              y=y,\n",
        "              output=output,\n",
        "              cost=cost,\n",
        "              optimizer=optimizer,\n",
        "              accuracy=accuracy,\n",
        "              summary=summaryppal\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqyRd-mtYMFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_neural_network(DNN,N_batch, n_personalities=\"four\", hm_epochs=10,batch_size=20):\n",
        "  mydir=\"/content/drive/My Drive/Colab Notebooks/ProyectoFinal/\"\n",
        "  if n_personalities==\"four\":\n",
        "    persntraductor=fourpersonalities\n",
        "    nameofdataset=\"Personality_\"\n",
        "  elif n_personalities==\"sixteen\":\n",
        "    persntraductor=sixteenpersonalities\n",
        "    nameofdataset=\"Personality_v2_\"\n",
        "  with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      writer = tf.summary.FileWriter(\"./LOG\")\n",
        "      tf.summary.FileWriter.add_graph(writer,sess.graph)\n",
        "      for epoch in range(hm_epochs):\n",
        "          epoch_loss = 0            \n",
        "          for NB in range(N_batch):\n",
        "                i=0\n",
        "                with open(mydir+nameofdataset+str(NB)+'.pickle','rb') as f:\n",
        "                    content=pickle.load(f)\n",
        "                    train_x=[cv2.resize(content[i],(56,56)) for i in range(len(content)) if (i%2)==0]\n",
        "                    train_y=[persntraductor(content[i]) for i in range(len(content)) if not (i%2)==0]\n",
        "                while i< len(train_x):\n",
        "                    start = i\n",
        "                    end = i+batch_size\n",
        "                    batch_x = train_x[start:end]\n",
        "                    batch_y = train_y[start:end]\n",
        "                    i+=batch_size                 \n",
        "                    feed_dict={DNN[\"x\"]: batch_x, \n",
        "                              DNN[\"y\"]: batch_y}\n",
        "                    _, c, prediction,y   = sess.run([DNN[\"optimizer\"], DNN[\"cost\"]\n",
        "                                                 , DNN[\"output\"], DNN[\"y\"]],\n",
        "                                                feed_dict=feed_dict)\n",
        "                    epoch_loss += c\n",
        "                    i+=batch_size\n",
        "\n",
        "          print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "          if (epoch%3==0):\n",
        "            s,accuracy,_,_=sess.run([DNN[\"summary\"],DNN[\"accuracy\"],DNN[\"output\"],DNN[\"y\"]],feed_dict={DNN[\"x\"]:batch_x,DNN[\"y\"]:batch_y})\n",
        "            writer.add_summary(s,epoch)\n",
        "            print (\"Accuracy in testing=\", accuracy)\n",
        "\n",
        "      accuracy,_,_=sess.run([DNN[\"accuracy\"],DNN[\"output\"],DNN[\"y\"]],feed_dict={DNN[\"x\"]:batch_x,DNN[\"y\"]:batch_y})\n",
        "      print (\"Accuracy  Train=\", accuracy)      \n",
        "      with open(mydir+nameofdataset+\"test.pickle\",'rb') as f:\n",
        "          content=pickle.load(f)\n",
        "          test_x=[cv2.resize(content[i],(56,56)) for i in range(len(content)) if (i%2)==0]\n",
        "          test_y=[persntraductor(content[i]) for i in range(len(content)) if not (i%2)==0]\n",
        "      prediction,y   = sess.run([DNN[\"output\"], DNN[\"y\"]], feed_dict={DNN[\"x\"]:test_x, DNN[\"y\"]:test_y})\n",
        "      correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "      print('Accuracy Test:',accuracy.eval())\n",
        "      confusion = tf.math.confusion_matrix(labels=np.argmax(y,axis=1),predictions=np.argmax(prediction,axis=1))\n",
        "      print(confusion.eval()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHrG-tSceFrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydir=\"/content/drive/My Drive/Colab Notebooks/ProyectoFinal/\"\n",
        "translatordic={}\n",
        "c=ord\n",
        "for NB in range(3):\n",
        "                i=0\n",
        "                with open(mydir+'Personality_'+str(NB)+'.pickle','rb') as f:\n",
        "                    content=pickle.load(f)\n",
        "                    train_x=[cv2.resize(content[i],(56,56)) for i in range(len(content)) if (i%2)==0]\n",
        "                    train_y=[content[i] for i in range(len(content)) if not (i%2)==0]\n",
        "                    for personality in train_y:\n",
        "                      if personality in translatordic.keys():\n",
        "                        translatordic[personality]+=1\n",
        "                      else:\n",
        "                        translatordic[personality]=1\n",
        "                        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIVmxkBd2PYs",
        "colab_type": "code",
        "outputId": "a375d96f-3ce8-4ced-84aa-f85232f75bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (translatordic)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Exploradores': 602, 'Centinelas': 875, 'Analistas': 661, 'Diplomaticos': 912, '': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0QQU9CvgGU4",
        "colab_type": "code",
        "outputId": "c5344b57-7d4d-4aa1-e6f5-47274762341f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "DNN=Neural_network_model(n_classes=4,batch_size=100)\n",
        "Example=train_neural_network(DNN,N_batch=3,n_personalities=\"four\",batch_size=100,hm_epochs=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 completed out of 20 loss: 1378195576.0\n",
            "Accuracy in testing= 0.1764706\n",
            "Epoch 1 completed out of 20 loss: 655632740.0\n",
            "Epoch 2 completed out of 20 loss: 437384871.5\n",
            "Epoch 3 completed out of 20 loss: 391918935.0\n",
            "Accuracy in testing= 0.47058824\n",
            "Epoch 4 completed out of 20 loss: 360470382.5\n",
            "Epoch 5 completed out of 20 loss: 300592704.0\n",
            "Epoch 6 completed out of 20 loss: 237884118.0\n",
            "Accuracy in testing= 0.64705884\n",
            "Epoch 7 completed out of 20 loss: 220347920.75\n",
            "Epoch 8 completed out of 20 loss: 238420728.5\n",
            "Epoch 9 completed out of 20 loss: 220981473.25\n",
            "Accuracy in testing= 0.8235294\n",
            "Epoch 10 completed out of 20 loss: 241449481.0625\n",
            "Epoch 11 completed out of 20 loss: 207973117.78125\n",
            "Epoch 12 completed out of 20 loss: 152825164.3125\n",
            "Accuracy in testing= 0.8235294\n",
            "Epoch 13 completed out of 20 loss: 114205298.640625\n",
            "Epoch 14 completed out of 20 loss: 108951264.5\n",
            "Epoch 15 completed out of 20 loss: 101581735.82421875\n",
            "Accuracy in testing= 1.0\n",
            "Epoch 16 completed out of 20 loss: 101879302.94140625\n",
            "Epoch 17 completed out of 20 loss: 106049763.5625\n",
            "Epoch 18 completed out of 20 loss: 105347286.875\n",
            "Accuracy in testing= 0.8235294\n",
            "Epoch 19 completed out of 20 loss: 82495390.03515625\n",
            "Accuracy  Train= 0.8235294\n",
            "Accuracy Test: 0.3362573\n",
            "[[47 18  3 18]\n",
            " [38 36  2 30]\n",
            " [27 17 10 35]\n",
            " [17 15  7 22]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96TQ9YiXALwP",
        "colab_type": "code",
        "outputId": "ed1f4c1a-c66c-4cd5-9926-ea9ae59e8bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "DNN=Neural_network_model(n_classes=16,batch_size=30)\n",
        "Example=train_neural_network(DNN,N_batch=3,n_personalities=\"sixteen\",batch_size=30,hm_epochs=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 completed out of 30 loss: 7065600220.0\n",
            "Epoch 1 completed out of 30 loss: 2696524340.0\n",
            "Epoch 2 completed out of 30 loss: 1954902862.0\n",
            "Epoch 3 completed out of 30 loss: 1609620814.0\n",
            "Epoch 4 completed out of 30 loss: 1373900762.0\n",
            "Epoch 5 completed out of 30 loss: 1207362061.0\n",
            "Epoch 6 completed out of 30 loss: 1049709092.0\n",
            "Epoch 7 completed out of 30 loss: 915963349.0\n",
            "Epoch 8 completed out of 30 loss: 855129524.5\n",
            "Epoch 9 completed out of 30 loss: 844438505.0\n",
            "Epoch 10 completed out of 30 loss: 681395824.0\n",
            "Epoch 11 completed out of 30 loss: 607003071.5\n",
            "Epoch 12 completed out of 30 loss: 525278260.0\n",
            "Epoch 13 completed out of 30 loss: 476478069.0\n",
            "Epoch 14 completed out of 30 loss: 402604451.25\n",
            "Epoch 15 completed out of 30 loss: 392454424.25\n",
            "Epoch 16 completed out of 30 loss: 352683186.25\n",
            "Epoch 17 completed out of 30 loss: 326842558.25\n",
            "Epoch 18 completed out of 30 loss: 289337705.1875\n",
            "Epoch 19 completed out of 30 loss: 251923611.25\n",
            "Epoch 20 completed out of 30 loss: 232956963.625\n",
            "Epoch 21 completed out of 30 loss: 192225244.125\n",
            "Epoch 22 completed out of 30 loss: 179762447.5625\n",
            "Epoch 23 completed out of 30 loss: 179222153.5\n",
            "Epoch 24 completed out of 30 loss: 199264216.75\n",
            "Epoch 25 completed out of 30 loss: 214867031.75\n",
            "Epoch 26 completed out of 30 loss: 213935318.78125\n",
            "Epoch 27 completed out of 30 loss: 166587975.9921875\n",
            "Epoch 28 completed out of 30 loss: 132856311.3125\n",
            "Epoch 29 completed out of 30 loss: 96144277.0859375\n",
            "Accuracy Train: 0.8666667\n",
            "Accuracy Test: 0.25730994\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 15  4  0  4  1  3  5  2  1  5]\n",
            " [ 0  2 12  2  1  4  4  3  1  2  2]\n",
            " [ 0  5 11  6  4  1  2  0  2  2  1]\n",
            " [ 0  6  2  3 15  1  1  6  1  3  1]\n",
            " [ 0  2 10  2  7  7  5  5  0  2  2]\n",
            " [ 0  3  5  3  5  2  5  4  1  0  0]\n",
            " [ 0 11  3  2  6  0  4 10  1  2  2]\n",
            " [ 0  5  6  2  8  3  1  4  7  1  0]\n",
            " [ 0  4  4  7  7  1  4  1  1  7  1]\n",
            " [ 0  5  0  0  0  0  1  1  0  0  4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPmHMW5kjFuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}